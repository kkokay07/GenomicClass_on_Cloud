{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkokay07/genomicclass/blob/master/ML_decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZSIxOIISqNr"
      },
      "source": [
        "# Cancer Classification using Decision Trees - Mathematical Approach\n",
        "\n",
        "## 1. Introduction to Decision Trees\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "Decision Trees use the following mathematical concepts:\n",
        "\n",
        "1) **Information Entropy**:\n",
        "   $$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "   where $p_i$ is the probability of class i in set S\n",
        "\n",
        "2) **Information Gain**:\n",
        "   $$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "   where $S_v$ is the subset of S where attribute A has value v\n",
        "\n",
        "3) **Gini Impurity**:\n",
        "   $$Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
        "\n",
        "### Why Decision Trees for SNP Analysis?\n",
        "- Non-linear relationships\n",
        "- Handle multiple classes naturally\n",
        "- Feature importance ranking\n",
        "- Easy interpretation\n",
        "- Handle categorical data well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpxOGYt3SqNv"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot example entropy and gini functions\n",
        "p = np.linspace(0, 1, 100)\n",
        "entropy = -p * np.log2(p + 1e-10) - (1-p) * np.log2(1-p + 1e-10)\n",
        "gini = 1 - (p**2 + (1-p)**2)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(p, entropy)\n",
        "plt.title('Binary Entropy Function')\n",
        "plt.xlabel('Probability')\n",
        "plt.ylabel('Entropy')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(p, gini)\n",
        "plt.title('Gini Impurity Function')\n",
        "plt.xlabel('Probability')\n",
        "plt.ylabel('Gini Impurity')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j_KoIE0SqNx"
      },
      "source": [
        "## 2. Data Loading and Understanding\n",
        "\n",
        "### SNP Data Structure\n",
        "SNP values are typically encoded as:\n",
        "- 1: Homozygous reference (AA)\n",
        "- 2: Heterozygous (AB)\n",
        "- 3: Homozygous alternate (BB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yCF0RCkSqNy"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('common_cancers.csv')\n",
        "\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Number of samples: {len(data)}\")\n",
        "print(f\"Number of SNPs: {len(data.columns)-1}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few rows of the data:\")\n",
        "display(data.head())\n",
        "\n",
        "# Plot distribution of cancer types\n",
        "plt.figure(figsize=(12, 6))\n",
        "cancer_counts = data.iloc[:, 0].value_counts()\n",
        "sns.barplot(x=cancer_counts.index, y=cancer_counts.values)\n",
        "plt.title('Distribution of Cancer Types')\n",
        "plt.xlabel('Cancer Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxO-g-AUSqNy"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "### Split and Prepare Data\n",
        "Unlike linear models, Decision Trees don't require feature scaling because they use threshold-based splitting rules:\n",
        "\n",
        "$$\\text{Split Rule}: X_i \\leq t$$\n",
        "\n",
        "where:\n",
        "- $X_i$ is feature i\n",
        "- $t$ is the threshold value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aEeYtY2SqNz"
      },
      "outputs": [],
      "source": [
        "# Split features and target\n",
        "X = data.iloc[:, 1:]  # SNP features\n",
        "y = data.iloc[:, 0]   # Cancer types\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Show unique values in features\n",
        "print(\"\\nUnique values in SNP features:\")\n",
        "print(pd.DataFrame(X_train).nunique().value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUgOEqBkSqNz"
      },
      "source": [
        "## 4. Model Training\n",
        "\n",
        "### Decision Tree Algorithm\n",
        "\n",
        "The tree is built recursively using these steps:\n",
        "\n",
        "1. For each feature $f$ and threshold $t$, calculate information gain:\n",
        "   $$IG(S, f, t) = H(S) - \\frac{|S_{left}|}{|S|}H(S_{left}) - \\frac{|S_{right}|}{|S|}H(S_{right})$$\n",
        "\n",
        "2. Choose the split that maximizes information gain:\n",
        "   $$(f^*, t^*) = \\arg\\max_{f,t} IG(S, f, t)$$\n",
        "\n",
        "3. Repeat recursively for child nodes until stopping criteria are met"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fBhdsCwSqN0"
      },
      "outputs": [],
      "source": [
        "# Create and train the model\n",
        "model = DecisionTreeClassifier(\n",
        "    criterion='entropy',     # Use information gain\n",
        "    max_depth=5,            # Limit tree depth to prevent overfitting\n",
        "    min_samples_split=20,   # Minimum samples required to split\n",
        "    min_samples_leaf=10     # Minimum samples in leaf nodes\n",
        ")\n",
        "\n",
        "print(\"Training the model...\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the tree structure\n",
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(model,\n",
        "          feature_names=X.columns,\n",
        "          class_names=model.classes_,\n",
        "          filled=True,\n",
        "          rounded=True)\n",
        "plt.title('Decision Tree Structure')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuMruVRGSqN0"
      },
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "### Key Metrics\n",
        "\n",
        "1. **Accuracy**:\n",
        "   $$\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$\n",
        "\n",
        "2. **Class-wise Precision**:\n",
        "   $$\\text{Precision}_i = \\frac{\\text{True Positives}_i}{\\text{True Positives}_i + \\text{False Positives}_i}$$\n",
        "\n",
        "3. **Class-wise Recall**:\n",
        "   $$\\text{Recall}_i = \\frac{\\text{True Positives}_i}{\\text{True Positives}_i + \\text{False Negatives}_i}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq18togRSqN0"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(conf_matrix,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=model.classes_,\n",
        "            yticklabels=model.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Cancer Type')\n",
        "plt.ylabel('Actual Cancer Type')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8pojJ6dSqN1"
      },
      "source": [
        "## 6. Feature Importance Analysis\n",
        "\n",
        "### Mathematical Interpretation\n",
        "\n",
        "Feature importance in decision trees is calculated based on the weighted impurity decrease:\n",
        "\n",
        "$$\\text{Importance}(f) = \\sum_{n \\in \\text{nodes using } f} w_n \\Delta I_n$$\n",
        "\n",
        "where:\n",
        "- $w_n$ is the weighted number of samples reaching node n\n",
        "- $\\Delta I_n$ is the impurity decrease at node n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j8JlJHMSqN1"
      },
      "outputs": [],
      "source": [
        "# Calculate feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'SNP': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Plot top 20 features\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=feature_importance.head(20), x='Importance', y='SNP')\n",
        "plt.title('Top 20 Most Important SNP Markers')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('SNP Marker')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print top 10 SNPs\n",
        "print(\"\\nTop 10 Most Important SNP Markers:\")\n",
        "print(feature_importance.head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
